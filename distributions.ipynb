{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peeling back the curtain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistics (of which machine learning is one part) is all about being able to make inferences about how the world works, given some data. In essence, trying to peel back the curtain on what is unknown. To be able to do that, we need a way to talk about what data are, and a way to talk about what could be behind that curtain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Talking about data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we observe 1000 patients in a hospital, each of whom has measurements of their pre-treatment blood pressure, heart rate, etc. and also their corresponding blood pressure after taking a drug. For a new patient who walks in the door, given their current vital signs, can we predict their post-treatment blood pressure? \n",
    "\n",
    "We need a way to talk abstractly about the data in these kinds of problems so that we can apply the same methods to different problems without having to completely rethink our approach every time.\n",
    "\n",
    "We're going to use a subscript $i$ to refer to data about the $i$th patient. So, for example, if $z$ were some measurement we made on all patients, $z_i$ would be the value of that measurement for the $i$th patient. The patient in our example is what we call the *unit of observation*. But if we were talking about classifying pictures as having cats or not having cats in them, the unit of observation would be the picture. So, in general, the subscript $i$ refers to the $i$th observation in the dataset, which could be a patient, a photograph, a credit card transaction, etc. \n",
    "\n",
    "The total number of observations is usually called $n$, so $i$ could stand for any number between 1 and $n$. A shorter way to write that is $i \\in \\{1, 2, \\dots n\\}$. You can read $\\in$ as \"in\", so this says \"$i$ is a number in the set of numbers that includes 1, 2, ... all the way up to $n$\". We'll use that notation for other things as well.\n",
    "\n",
    "We usually use the variable $y$ to refer to the measurement we are trying to predict, which is in this case the patient's post-treatment blood pressure. You'll most often see $y$ called the *target*, or *outcome*. Using our observation subscripts, $y_1$ would be the first patient's post-treatment blood pressure, $y_2$ would be the second patient's blood pressure, etc. \n",
    "\n",
    "We can do the same thing for the other measurements, which are called *predictors*, *features*, or *covariates*. We could use different letters for each predictor, e.g. pre-treatment blood pressure would be $z$, pre-treatment heart rate would be $x$, but the standard practice is to call them all \"$x$\", but subscript them with an index $j$ (different than the index $i$ that refers to the unit of observation). So, in our example, we'll say $x_{i1}$ is patient $i$'s pre-treatment blood pressure, $x_{i2}$ is their pre-treatment heart rate, etc. We will use $p$ to refer to the number of predictors we have for each observation, so $j \\in \\{1 \\dots p\\}$. We'll use $x_i$ to refer to a list of all of the features for observation $i$:\n",
    "\n",
    "$$\n",
    "x_i = \n",
    "\\left[\n",
    "\\begin{array}{cccc}\n",
    "x_{i1} & x_{i2} & \\cdots & x_{ip}\n",
    "\\end{array}\n",
    "\\right] \n",
    "$$\n",
    "\n",
    "\n",
    "Now each observation is decribed by its target $y_i$ and its vector of predictors $x_i$. We stack these measurements together into lists of outcomes and predictors so we can refer to all of them at once:\n",
    "\n",
    "$$\n",
    "X = \n",
    "\\left[\n",
    "\\begin{array}{c}\n",
    "x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n\n",
    "\\end{array}\n",
    "\\right] \n",
    "=\n",
    "\\left[\n",
    "\\begin{array}{cccc}\n",
    "x_{11} & x_{12} & \\cdots & x_{1p} \\\\\n",
    "x_{21} & x_{22} & \\cdots & x_{2p} \\\\\n",
    "\\vdots & & \\ddots & \\vdots \\\\\n",
    "x_{n1} & x_{n2} & \\cdots & x_{np} \n",
    "\\end{array}\n",
    "\\right]\n",
    "\\quad\n",
    "Y = \n",
    "\\left[\n",
    "\\begin{array}{c}\n",
    "y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n\n",
    "\\end{array}\n",
    "\\right] \n",
    "$$\n",
    "\n",
    "You may recall that ordered lists of numbers are also called *vectors*, and a list of lists of numbers (like $X$) can be written as a *matrix*. These are the terms we'll use going forward. \n",
    "\n",
    "Note that each column of $X$ (which we'll call $x_{:j}$) is a vector that contains every measurement of the $j$th feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can formalize our learning problem in the abstract. All the measurements for the existing patients are contained in the matrix of predictors $X$ and the vector of targets $Y$. A new patient walks in the door (patient #$n+1$), and we measure all their predictors $x_{n+1}$. What we don't observe, but want to estimate, is the value of their target, $y_{n+1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example in Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll make some fake data in python to demonstrate. The `numpy` library is used to represent vectors and matrices in python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are all the predictors and outcomes for $n$ observations (e.g. pre-treatment measurements and post-treatment blood pressure for $n$ patients):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.51492805, 0.36812043],\n",
       "        [0.84000769, 0.33231569],\n",
       "        [0.45047613, 0.07787864],\n",
       "        [0.11453122, 0.88784924],\n",
       "        [0.03765258, 0.92568634]]),\n",
       " array([0.26450185, 0.26007052, 0.64423337, 0.41277037, 0.26557173]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "n = 5 # 5 observations\n",
    "p = 2 # 2 features\n",
    "X = np.random.rand(n,p) # a random matrix of numbers between 0 and 1. \n",
    "y = np.random.rand(n)\n",
    "X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here are the predictors for a new observation (e.g. the pre-treatment measurements for a new patient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.85688858, 0.41232152]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_new = np.random.rand(1,p)\n",
    "x_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we want is an algorithm that takes any values of `x_new` and uses all the data in `X` and `y` to output an prediction for what `y_new` should be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fancy_machine_learning_prediction(x_new):\n",
    "    # ... do some stuff with X and y\n",
    "    return y_new_estimated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Aside about non-tabular data, feature engineering, and methods for nontabular data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's behind the curtain?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, we demonstrated some code that generates fake data $X$ and $Y$. On the other hand, real data comes from the real world, not from some python code. For every dataset, there is an immensely complex network of causal interactions that ultimately \"produces\" the data. \n",
    "\n",
    "For example, in our blood pressure example, a patient's pre-treatment vital signs are caused by their physiological state: their genetics, life history, what they ate for breakfast that morning, whether or not they just ran up a flight of stairs, and so on and so forth. Taking a drug influences the levels of certain chemicals in the blood, which are taken up at particular rates in certain organs by certain enzymes, the levels of which are impacted by the patient's genetics and prior physiological state, which was influenced by their life history, etc. Thus the impact of the drug on cellular processes is mediated by these factors. The cells respond by increasing or decreasing their production of some proteins or metabolites, which, in combination with the immediate condition of the patient when the measurement is taken, determines the post-treatment blood pressure. \n",
    "\n",
    "Or, let's say we're trying to determine whether or not there is a cat in a photograph. The cat being in front of the camera when the photo was taken ($y_i$) could be caused by a huge number of factors, and the values of the pixels in the photograph ($x_i$) are caused by the reflection of photons emitted from sources of light off the cat (and other objects) and the mechanics of the detection of light inside the camera.\n",
    "\n",
    "In a nutshell, the world is complicated. There is no way that mere mortals could ever write code accurate enough to perfectly simulate the exact processes that produce data about complex real-world phenomena.\n",
    "\n",
    "But, despite the complexity, you should start thinking about that complex web of causality as \"code\" that's being run in some cosmic simulation. Maybe you can imagine that there are \"data gods\" that write and are running this code. We'll never see their code, and we'll never be able to understand it, but somewhere, out there, that metaphysical code is running, and it's generating the observations that we see in our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can think of that code as a little \"factory\" that pumps out observations of $x_i$ and $y_i$, one at a time. The factory is behind a curtain that we can't ever look behind, but we can see the pile of $x_i$s and $y_i$s that come out of it, which are our $X$ and $Y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Figure of factory and curtain and boxes üè≠ üì¶ üì¶ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we had that code, we'd be able to reverse engineer it to predict $y_i$ given $x_i$ as accurately as would be \n",
    "possible with those predictors. In practice, however, we can only build a *model* of that code. Our model will never capture the complexities of reality, the same way that a model plane doesn't even begin to approach the complexity of a real aircraft. But, ideally, it will be similar enough in ways that are important for the task at hand: if we're using a model plane just to demonstrate what an aircraft migth look like, we don't need the model to have functioning jet engines. And if all we need to do is predict $y_i$ for a new $x_i$, we don't exactly need to understand the complex web of causality linking the two together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do, however, need a way to talk about the relationship that $x_i$ and $y_i$ might have. And to do that, we need a way to talk abstractly about the \"code\" that's behind the curtain, the same way we developed abstract terms to describe our data. Thankfully, the language of probability works perfectly for that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random variables are factories that generate data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's talk about the simplest data factory possible. We'll call our factory $\\mathbf z$. This factory pushes out one value $z_i$ at a time. Half the time you get a $1$ and half the time you get a $0$; those are the only values that the $\\mathbf z$ factory can produce. And the factory is built to reset itself between producing each value, so whatever $z_i$ is has no impact on $z_{i+1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the language of probability theory, $z_i$ are *realizations* from the *random variable* $\\mathbf z$, which has a *distribution*:\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "P(\\mathbf z = 0) &=& 1/2 \\\\ \n",
    "P(\\mathbf z = 1) &=& 1/2\n",
    "\\end{array}\n",
    "\\quad \\quad \\text{or} \\quad \\quad\n",
    "P(\\mathbf z=z) =\n",
    "\\begin{cases}\n",
    "1/2 & \\text{for }z=0 \\\\\n",
    "1/2 & \\text{for }z=1\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we've been loosely calling a \"factory\" is a *random variable* in the language of probability theory. But that's just a name. You can keep thinking of them as factories, or code, that generate data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b> \n",
    "Random variables are often written in uppercase, (e.g. Z) and their realizations in lowercase (z). We're going to be using uppercase for matrices (and sets), so I'm going to use boldface ($\\mathbf z$) to denote random variables instead. Be aware of that convention.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so if the random variable is a factory, and the realizations of the random variable are the output of that factory (the data we get to see), then how do we read a statement like $P(\\mathbf z = 0) = 1/2$? Well, that just means that the value $z$ that $\\mathbf z$ produces is $0$ half of the time. But what exactly do we mean by \"half the time\"? While we usually don't have to think deeper than this, you'll see later that it is sometimes necessary to have a more rigorous definition of probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build that definition. We'll start with some raw materials. All factories have raw materials that go into them, which end up being turned into the finished product. In a similar way, random variables have inputs which get mapped to realized values. We'll call them \"data ore\": the unrefined precursor that gets transformed by our factory (random variable $\\mathbf z$)into the data product $z$. The data ore exists in units (data ore nuggets). The factory takes one nugget at a time and transforms it into a value.\n",
    "\n",
    "The nuggets are kept in an big silo called $\\Omega$ before they go to $\\mathbf z$. This silo is filled to the brim with *all* of the possible nuggets that could be fed into the factory, one of each of them. It's also a magic silo, so when you take out a nugget, another one exactly like it is mined out of the depths of the cosmos to take its place in the silo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each nugget is gets transformed into a value of $z$, but the process isn't random. For instance, if a nugget named \"Karl\" turned into a 1 when fed through $\\mathbf z$, then we would *always* get a 1 when Karl goes into $\\mathbf z$. But we know that sometimes $\\mathbf z$ produces 0s, so there must be other nuggets whose destiny is to become 0s, just like Karl's destiny is to be a 1. The \"randomness\" in $\\mathbf z$ isn't caused by what's in the factory, it's caused by randomly picking a nugget to throw into it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even code up our little example, imagining that we have 10 nuggets, boringly named \"0\", \"1\", \"2\"... \"9\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "Œ© = set([0,1,2,3,4,5,6,7,8,9]) # sample space (silo) of outcomes (ore nuggets)\n",
    "\n",
    "def Z(œâ): # factory (random variable)- I'll use uppercase in code for ùê≥\n",
    "    if œâ in set([1,4,5,8,9]): # these are the outcomes (nuggets) that map to the value 1 \n",
    "        return 1\n",
    "    if œâ in set([0,2,3,6,7]): # these are the outcomes (nuggets) that map to the value 0 \n",
    "        return 0\n",
    "    \n",
    "import random\n",
    "def realize(rand_var, Œ©): # run the assembly line!\n",
    "    œâ = random.sample(Œ©, 1).pop() # grab a single nugget out of the silo at random\n",
    "    return rand_var(œâ) # push it through the factory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are 20 observations $z=[z_1, z_2, \\dots z_{20}]$, fresh off the assembly line of the $\\mathbf z$ factory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = [realize(Z, Œ©) for i in range(20)] \n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b> \n",
    "\n",
    "You may sometimes also see notation like $\\mathbf z_i \\overset{IID}{\\sim} \\mathbf z$. This means that $\\mathbf z_i$ is a perfect copy of the factory $\\mathbf z$ (we say $\\mathbf z_i$ are <i>independent and identically distributed</i>, or IID). \n",
    "    \n",
    "$z_i$ is now the single product of the $i$th factory $\\mathbf z_i$ instead of the $i$th product of the one factory $\\mathbf z$. To get our data $z = [z_1, z_2, \\dots z_n]$, we run each factory once, instead of running the prototype factory $\\mathbf z$ $n$ times. As far as the data you get are concerned, there's no difference between doing it one way or the other, but you should be familiar with the $\\mathbf z_i \\overset{IID}{\\sim} \\mathbf z$ notation.\n",
    "   \n",
    "Here's the equivalent code:\n",
    "```\n",
    "ùê≥s = [ùê≥ for i in range(5)] # ùê≥_i ~IID~ ùê≥\n",
    "z = [realize(ùê≥i, Œ©) for ùê≥i in ùê≥s] # sample once from each z_i\n",
    "```\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another picture of the factory, the silo, data nuggets, z, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to define probability: the probability of an *event* (a value $z$) is just the proportion of the silo that's taken up by nuggets that are destined to become that value $z$ when fed through $\\mathbf z$. That's it. We denote that proportion with the notation $P(\\mathbf z = z)$. In our example above, saying $P(\\mathbf z = 1) = 1/2$ means that half of all the possible nuggets that could go into $\\mathbf z$ would produce a 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a definition we can code up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def P(rand_var, realization):\n",
    "    A = set(œâ for œâ in Œ© if rand_var(œâ) in realization) # what are all the nuggets that map to the value(s) in question?\n",
    "    return len(A)/len(Œ©) # what is the \"volume\" of those nuggets relative to the volume of the silo Œ©?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5, 0.5)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P(Z,[0]), P(Z,[1]) # P(z=0), P(z=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strictly speaking, the silo $\\Omega$ is called a *sample space* and the data ore nuggets are called *outcomes* (not to be confused with what we call the variable we want to predict in machine learning). A random variable is defined as a function that maps an element of $\\Omega$ to a realization $z$. The probability of an event $z$ is the *measure* (volume, or proportion of total volume) of the set of outcomes (data ore nuggets) that map to $z$ (are destined to be transformed to $z$ by $\\mathbf z$). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRTCXQ098kIFldwWw8VEPSR_q9Tbk1BFjuhFH8V8NVPskxrtVj7&s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're looking at this and thinking that I can't possibly be serious, that the foundations of statistics and machine learning can't possibly be built up from imagining data factories and magical silos... well, you're wrong. Sure, I've concocted a somewhat elaborate metaphor, but it's a metaphor that accurately describes how these otherwise very abstract concepts relate to each other. If you can look at something like $P(\\mathbf z = z) := P(\\{\\omega \\in \\Omega \\vert \\mathbf z(w)=z\\})$ and immediately come away with an understanding of what that means, all the more power to you. But I don't. At least not without first building up an intuition for each of the components.\n",
    "\n",
    "When I talk about these things outside of the context of explaining them, I do call them by their real names (e.g. random variable, sample space, etc.) because that's what people have called them for nearly a century. But when I close my eyes and *reason* about these concepts, I'm thinking about something tangible, like a factory. As we go on I'm going to introduce more mathematical notation as we need it, and I'm going to wean off the factory metaphor, but I encourage you to keep building your intuition about these concepts instead of thinking about them as abstract symbols on a page. The symbols are just a convenient shorthand for the ideas. The only reason to know the standard names and symbols is to be able to read and understand what others have written. If you find yourself skimming over an equation- stop. Read it slowly and think about what each part means.\n",
    "\n",
    "So now that we're here, let's peel that equation apart and demystify the notation! Here it is again:\n",
    "\n",
    "$$P(\\mathbf z = z) := P(\\{\\omega \\in \\Omega \\vert \\mathbf z(w)=z\\})$$\n",
    "\n",
    "To start, the $:=$ means \"the thing on the left is defined as the thing on the right\". So we're saying that when we write \"$P(\\mathbf z = z)$\", we really mean whatever \"$P(\\{\\omega \\in \\Omega \\vert \\mathbf z(w)=z\\})$\" is. Ok, next up is [set-builder notation](https://www.mathsisfun.com/sets/set-builder-notation.html): you can read $\\{a\\in A | f(a) = 1\\}$ as \"the collection of all the elements $a$ in the set $A$ *such that* $f(a)=1$\". So $\\{\\omega \\in \\Omega \\vert \\mathbf z(w)=z\\}$ is the set of outcomes $\\omega$ that become $z$ when passed through the random variable $\\mathbf z$. Finally, we have $P()$, which is the [probability measure](https://en.wikipedia.org/wiki/Probability_measure). We won't define this super rigorously, but it is basically defined as the measurement of the proportion of all of the outcomes in $\\Omega$ that are contained in the subset $\\{\\omega \\in \\Omega \\vert \\mathbf z(w)=z\\}$. If you put all of that together, you'll see that it's exactly the same as the definition we put together using our factory anaology.\n",
    "\n",
    "When we say \"the proportion of all outcomes in $\\Omega$ that are in a subset $A$\", what we mean is that we're literally going to measure the volume of space that the nuggets in $A$ take up in the silo $\\Omega$. By convention, we say that $\\Omega$ has volume 1 so that the volume of $A$ is also the proportion of $A$ in $\\Omega$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok- I promised that it would be useful to define probability in a more rigorous way than \"$z$ happens $x$% of the time\". Now we're going to see why. \n",
    "\n",
    "To start with, let's \"derive\" a relatively simple fact: for a subset $F \\subset \\Omega$, $P(F) \\in [0,1]$. This is a compact way of writing that for any subset of outcomes, the proportion of outcomes in that subset relative to how many are in the whole sample space is a number between 0 and 1 ($A \\subset B$ means that $A$ is a subset of $B$). At this point, that should be obvious to you. If the volume of our silo $\\Omega$ is 1, the volume of any subset of that has to be less than 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a trickier one: for $A,B \\subset \\Omega$, if $A \\cap B = 0$, then $P(A \\cup B) = P(A) + P(B)$. This one has more symbols, but it's also really obvious when you know what they mean. $A \\cap B$ is read as \"the intersection of the sets $A$ and $B$\", which is the set of elements that are in both sets. It's the middle part of a Venn diagram. $A \\cup B$ is read  as \"the union of $A$ and $B$\", which is all of the elements in either set- that's the entirety of the Venn diagram. So what we're saying here is that if A and B don't share any outcomes between them, the probability of the union of the two is the same as the sum of the probabilities of each of them. \n",
    "\n",
    "That also seems cryptic until you think about it in terms of quantities of ore nuggets that produce certain values when fed through the factory. If you take all the ore nuggets that end up becoming the value $a$ (call that set of nuggets $A$), and all the nuggets that end up becoming the value $b$ (call that $B$), then the total volume that end up becoming either $a$ or $b$ (that's $P(A \\cup B)$) is the sum of the volumes that become $a$ (that's $P(A)$) and those that become $b$ (that's $P(B)$). \n",
    "\n",
    "Note that this only holds for sets $A$ and $B$ that don't overlap ($A\\cap B = 0$). For instance, if $A= \\{\\omega \\vert \\mathbf z(\\omega) \\in \\{0,1\\}\\}$ (all the outcomes that map to either 0 or 1) and $B = \\{\\omega \\vert \\mathbf z(\\omega) = 0\\}$ (all the outcomes that map to 0), every element of $B$ is also an element of $A$, so the volume of the union $A \\cup B$ is the volume of $A$, which is not the volume of $A$ plus the volume of $B$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even use our code from before to demonstrate this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = set([0])\n",
    "B = set([1])\n",
    "P(Z, A) + P(Z, B) == P(Z, A|B) # in python, set intersection ‚à© is written | because an element is in A‚à©B if it is in A OR B (A|B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = set([0,1])\n",
    "B = set([0])\n",
    "P(Z,A) + P(Z,B) == P(Z, A|B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This last point brings up the fact that we can also talk about quantities like $P(\\mathbf z \\in F)$, where $F$ is some set of possible realizations, like $\\{0,1\\}$. That's more general than the probability at a single value $z$: $P(\\mathbf z = z)$. All we need to do is count up the volume of all the nugets that produce any of the values that's in $F$, instead of just the nuggets that produce $z$. Our code is already set up to do that- we used it to calculate `P(Z, set([0,1]))`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Exercise:</b> \n",
    "\n",
    "Write code for a new random variable $\\mathbf w$ that behaves like this:\n",
    "    \n",
    "$$\n",
    "P(\\mathbf w=w) =\n",
    "\\begin{cases}\n",
    "1/6 & \\text{for }w=-1 \\\\\n",
    "1/3 & \\text{for }w=0 \\\\\n",
    "1/2 & \\text{for }z=1\n",
    "\\end{cases}\n",
    "$$\n",
    "    \n",
    "You'll need to make your own sample space `Œ©` and define the function `w_rand_var(œâ)`. Test it out using the `realize()` and `P()` functions we wrote. Use `P()` to calculate $P(\\mathbf w \\in \\{-1,0\\})$.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b> \n",
    "\n",
    "This is as pretty much as rigorous as probability theory gets. The only other wrinkle is that when we start talking about random variables that can produce things like continuous values (of which there are an infinite number), technically we need to consider silos that are filled with infinite numbers of nuggets (that still all together occupy a volume of 1!). And, fun fact, it turns out that in this case there are some sets of nuggets that are distributed in such a messed up way within the silo that it is impossible to measure their volume. But they are of no practical importance and probabilists just ignore these sets and say that their probability is not defined (no joke)!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random variables are described by their probability distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- what the nuggets are doesn't really matter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate random variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \"joint\" vs mulitvariate RVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
