{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peeling back the curtain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistics (of which machine learning is one part) is all about being able to make inferences about how the world works given data. In essence, trying to peel back the curtain on what is unknown. To be able to do that, we need a way to talk about what data are, and a way to talk about what could be behind that curtain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Talking about data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we observe 1000 patients in a hospital, each of whom has measurements of their pre-treatment blood pressure, heart rate, etc. and also their corresponding blood pressure after taking a drug. For a new patient who walks in the door, given their current vital signs, can we predict their post-treatment blood pressure? \n",
    "\n",
    "We need a way to talk abstractly about the data in these kinds of problems so that we can apply the same methods to different problems without having to completely rethink our approach every time.\n",
    "\n",
    "We're going to use a subscript $i$ to refer to data about the $i$th patient. So, for example, if $z$ were some measurement we made on all patients, $z_i$ would be the value of that measurement for the $i$th patient. The patient in our example is what we call the *unit of observation*. But if we were talking about classifying pictures as having cats or not having cats in them, the unit of observation would be the picture. So, in general, the subscript $i$ refers to the $i$th observation in the dataset, which could be a patient, a photograph, a credit card transaction, etc. \n",
    "\n",
    "The total number of observations is usually called $n$, so $i$ could stand for any number between 1 and $n$. A shorter way to write that is $i \\in \\{1, 2, \\dots n\\}$. You can read $\\in$ as \"in\", so this says \"$i$ is a number in the set of numbers that includes 1, 2, ... all the way up to $n$\". We'll use that notation for other things as well.\n",
    "\n",
    "We usually use the variable $y$ to refer to the measurement we are trying to predict, which is in this case the patient's post-treatment blood pressure. You'll most often see $y$ called the *target*, or *outcome*. Using our observation subscripts, $y_1$ would be the first patient's post-treatment blood pressure, $y_2$ would be the second patient's blood pressure, etc. \n",
    "\n",
    "We can do the same thing for the other measurements, which are called *predictors*, *features*, or *covariates*. We could use different letters for each predictor, e.g. pre-treatment blood pressure would be $z$, pre-treatment heart rate would be $x$, but the standard practice is to call them all \"$x$\", but subscript them with an index $j$ (different than the index $i$ that refers to the unit of observation). So, in our example, we'll say $x_{i1}$ is patient $i$'s pre-treatment blood pressure, $x_{i2}$ is their pre-treatment heart rate, etc. We will use $p$ to refer to the number of predictors we have for each observation, so $j \\in \\{1 \\dots p\\}$. We'll use $x_i$ to refer to a list of all of the features for observation $i$:\n",
    "\n",
    "$$\n",
    "x_i = \n",
    "\\left[\n",
    "\\begin{array}{cccc}\n",
    "x_{i1} & x_{i2} & \\cdots & x_{ip}\n",
    "\\end{array}\n",
    "\\right] \n",
    "$$\n",
    "\n",
    "\n",
    "Now each observation is decribed by its target $y_i$ and its vector of predictors $x_i$. We stack these measurements together into lists of outcomes and predictors so we can refer to all of them at once:\n",
    "\n",
    "$$\n",
    "X = \n",
    "\\left[\n",
    "\\begin{array}{c}\n",
    "x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n\n",
    "\\end{array}\n",
    "\\right] \n",
    "=\n",
    "\\left[\n",
    "\\begin{array}{cccc}\n",
    "x_{11} & x_{12} & \\cdots & x_{1p} \\\\\n",
    "x_{21} & x_{22} & \\cdots & x_{2p} \\\\\n",
    "\\vdots & & \\ddots & \\vdots \\\\\n",
    "x_{n1} & x_{n2} & \\cdots & x_{np} \n",
    "\\end{array}\n",
    "\\right]\n",
    "\\quad\n",
    "Y = \n",
    "\\left[\n",
    "\\begin{array}{c}\n",
    "y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n\n",
    "\\end{array}\n",
    "\\right] \n",
    "$$\n",
    "\n",
    "You may recall that ordered lists of numbers are also called *vectors*, and a list of lists of numbers (like $X$) can be written as a *matrix*. These are the terms we'll use going forward. \n",
    "\n",
    "Note that each column of $X$ (which we'll call $x_{:j}$) is a vector that contains every measurement of the $j$th feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can formalize our learning problem in the abstract. All the measurements for the existing patients are contained in the matrix of predictors $X$ and the vector of targets $Y$. A new patient walks in the door (patient #$n+1$), and we measure all their predictors $x_{n+1}$. What we don't observe, but want to estimate, is the value of their target, $y_{n+1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example in Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll make some fake data in python to demonstrate. The `numpy` library is necessary to represent vectors and matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are all the predictors and outcomes for $n$ observations (e.g. pre-treatment measurements and post-treatment blood pressure for $n$ patients):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.51492805, 0.36812043],\n",
       "        [0.84000769, 0.33231569],\n",
       "        [0.45047613, 0.07787864],\n",
       "        [0.11453122, 0.88784924],\n",
       "        [0.03765258, 0.92568634]]),\n",
       " array([0.26450185, 0.26007052, 0.64423337, 0.41277037, 0.26557173]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "n = 5 # 5 observations\n",
    "p = 2 # 2 features\n",
    "X = np.random.rand(n,p) # a random matrix of numbers between 0 and 1. \n",
    "y = np.random.rand(n)\n",
    "X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here are the predictors for a new observation (e.g. the pre-treatment measurements for a new patient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.85688858, 0.41232152]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_new = np.random.rand(1,p)\n",
    "x_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we want is an algorithm that takes any values of `x_new` and uses all the data in `X` and `y` to output an prediction for what `y_new` should be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fancy_machine_learning_prediction(x_new):\n",
    "    # ... do some stuff with X and y\n",
    "    return y_new_estimated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Aside about non-tabular data, feature engineering, and methods for nontabular data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's behind the curtain?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, we demonstrated some code that generates fake data $X$ and $y$. On the other hand, real data comes from the real world, not from some python code. For every dataset, there is an immensely complex network of causal interactions that ultimately \"produces\" the data. \n",
    "\n",
    "For example, in our blood pressure example, a patient's pre-treatment vital signs are caused by their physiological state: their genetics, life history, what they ate for breakfast that morning, whether or not they just ran up a flight of stairs, and so on and so forth. Taking a drug influences the levels of certain chemicals in the blood, which are taken up at particular rates in certain organs by certain enzymes, the levels of which are impacted by the patient's genetics and prior physiological state, which was influenced by their life history, etc. Thus the impact of the drug on cellular processes is mediated by these factors. The cells respond by increasing or decreasing their production of some proteins or metabolites, which, in combination with the immediate condition of the patient when the measurement is taken, determines the post-treatment blood pressure. \n",
    "\n",
    "Or, let's say we're trying to determine whether or not there is a cat in a photograph. The cat being in front of the camera when the photo was taken ($y_i$) could be caused by a huge number of factors, and the values of the pixels in the photograph ($x_i$) are caused by the reflection of photons emitted from sources of light off the cat (and other objects) and the mechanics of the detection of light inside the camera.\n",
    "\n",
    "In a nutshell, the world is complicated. There is no way that mere mortals could ever write code accurate enough to perfectly simulate the exact processes that produce data about complex real-world phenomena.\n",
    "\n",
    "But, despite the complexity, you should start thinking about that complex web of causality as \"code\" that's being run in some cosmic simulation. Maybe you can imagine that there are \"data gods\" that write and are running this code. We'll never see their code, and we'll never be able to understand it, but somewhere, out there, that metaphysical code is running, and it's generating the observations that we see in our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can think of that code as a little \"factory\" that pumps out observations of $x_i$ and $y_i$, one at a time. The factory is behind a curtain that we can't ever look behind, but we can see the pile of $x_i$s and $y_i$s that come out of it, which are our $X$ and $Y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Figure of factory and curtain and boxes üè≠ üì¶ üì¶ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we had that code, we'd be able to reverse engineer it to predict $y_i$ given $x_i$ as accurately as would be \n",
    "possible with those predictors. In practice, however, we can only build a *model* of that code. Our model will never capture the complexities of reality, the same way that a model plane doesn't even begin to approach the complexity of a real aircraft. But, ideally, it will be similar enough in ways that are important for the task at hand: if we're using a model plane just to demonstrate what an aircraft migth look like, we don't need the model to have functioning jet engines. And if all we need to do is predict $y_i$ for a new $x_i$, we don't exactly need to understand the complex web of causality linking the two together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do, however, need a way to talk about the relationship that $x_i$ and $y_i$ might have. And to do that, we need a way to talk abstractly about the \"code\" that's behind the curtain, the same way we developed abstract terms to describe our data. Thankfully, the language of probability works perfectly for that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability distributions are code for generating data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's talk about the simplest data factory possible. We'll call our factory $\\mathbf z$. This factory pushes out one value $z_i$ at a time. Half the time you get a $1$ and half the time you get a $0$; those are the only values that the $\\mathbf z$ factory can produce. And the factory is built to reset itself between producing each value, so whatever $z_i$ is has no impact on $z_{i+1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the language of probability thoery, $z_i$ are *realizations* from the *random variable* $\\mathbf z$, which has a *distribution*:\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "P(\\mathbf z = 0) &=& 1/2 \\\\ \n",
    "P(\\mathbf z = 1) &=& 1/2\n",
    "\\end{array}\n",
    "\\quad \\quad \\text{or} \\quad \\quad\n",
    "P(\\mathbf z=z) =\n",
    "\\begin{cases}\n",
    "1/2 & \\text{for }z=0 \\\\\n",
    "1/2 & \\text{for }z=1\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may sometimes also see notation like $\\mathbf z_i \\overset{IID}{\\sim} \\mathbf z$. This means that $\\mathbf z_i$ is a factory made from the blueprints of the factory $\\mathbf z$ (we say $\\mathbf z_i$ are *independent and identically distributed*, or IID). $z_i$, then is the number that comes out of the $\\mathbf z_i$ factory, and we run each factory only once instead of running the $\\mathbf z$ factory $n$ times. As far as the data you get are concerned, there's no difference, but you should be familiar with that notation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we've been loosely calling a \"factory\" is a *random variable* in the language of probability theory. But that's just a name. You can keep thinking of them as factories, or code, that generate data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so if the random variable is a factory, and the realizations of the random variable are the output of that factory (the data we get to see), then how do we read a statement like $P(\\mathbf z = 0) = 1/2$? Well, that just means that the value $z$ that $\\mathbf z$ produces is $0$ half of the time. But what exactly do we mean by \"half the time\"? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, all factories have raw materials that go into them, which end up being turned into the finished product. In a similar way, random variables have inputs which get mapped to realized values. These inputs don't really matter except to define probability in a rigorous way, so we're not going to talk about them too much or call them by a fancy mathematical name. We'll just call them \"data ore\": the unrefined precursor that gets transformed by our factory (random variable $\\mathbf z$)into the data product $z$. The data ore exists in units (data ore nuggets). The factory takes one nugget at a time and transforms it into a value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each nugget is deterministically transformed into a value of $z$. For instance, if a nugget named \"Karl\" turned into a 1 when fed through $\\mathbf z$, then we would *always* get a 1 when Karl goes into $\\mathbf z$. But we know that sometimes $\\mathbf z$ produces 0s, so there must be other nuggets whose destiny is to become 0s, just like Karl's destiny is to be a 1. The \"randomness\" in $\\mathbf z$ isn't caused by what's in the factory, it's caused by randomly picking a nugget to throw into it.\n",
    "\n",
    "The nuggets are kept in an big silo called $\\Omega$ before they go to $\\mathbf z$. This silo is filled to the brim with *all* of the possible nuggets that could be fed into the factory, one of each of them. Now we're ready to define probability: the probability of an *event* (a value $z$) is just the proportion of the silo that's taken up by nuggets that are destined to become that value $z$ when fed through $\\mathbf z$. That's it. We denote that proportion with the notation $P(\\mathbf z = z)$. In our example above, saying $P(\\mathbf z = 1) = 1/2$ means that half of all the possible nuggets that could go into $\\mathcal z$ would produce a 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strictly speaking, the silo $\\Omega$ is called a *sample space* and the data ore nuggets are called *outcomes* (not to be confused with what we call the variable we want to predict in machine learning). A random variable is defined as a function that maps an element of $\\Omega$ to an element of $\\mathcal E$, the *event space*, which contains all the possible realizations that the random variable can attain ($\\mathcal E = \\{0,1\\}$ in our example). The probability of an event $z$ is the *measure* (volume, or proportion of total volume) of the set of outcomes (data ore nuggets) that map to $z$ (are destined to be transformed to $z$ by $\\mathbf z$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another picture of the factory, the silo, data nuggets, z, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
